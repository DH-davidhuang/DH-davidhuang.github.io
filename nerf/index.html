<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 5: Diffusion</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            background-color: #f9f9f9;
        }
        header {
            background-color: #0078D7;
            color: white;
            padding: 20px;
            text-align: center;
        }
        section {
            margin: 20px;
            padding: 20px;
            background-color: white;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        h1, h2, h3 {
            color: #333;
        }
        .image-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 20px;
            margin-top: 20px;
        }
        .image-container img {
            max-width: 45%;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }
        .image-container figure {
            margin: 0;
            text-align: center;
        }
        .image-container figcaption {
            margin-top: 10px;
            font-size: 0.9em;
            color: #555;
        }
    </style>
<section>
    <h2>Part 1 and 2: NERF!</h2>
    <p>
        As a reference, the images below show the process of optimizing the network to fit on the given images. 
        The following results are for a neural field trained on a single input image (the fox), as well as a second example (the bear).
    </p>
    <h3>Model Architecture and Hyperparameters</h3>
    <ul>
        <li>Positional Encoding: L = 10 (total input dimension after encoding: 42)</li>
        <li>MLP Architecture: MLPNeuralField with hidden_dim=256, output_dim=3, num_layers=3</li>
        <li>Loss Function: MSELoss</li>
        <li>Optimizer: Adam with LR=0.001</li>
        <li>Number of Epochs: 1000</li>
    </ul>

    <h3>PSNR Over Training (Fox)</h3>
    <p>Below is the PSNR curve over epochs for the "fox" image training run:</p>
    <figure>
        <img src="media/part_1/fox_pnsr_pic.png" alt="Fox PSNR over epochs" />
        <figcaption>Figure: Fox PSNR Curve</figcaption>
    </figure>
    
    <p>Below is the training loss over epochs for the "fox" image:</p>
    <figure>
        <img src="media/part_1/fox_loss_over_epochs.png" alt="Fox Loss over epochs" />
        <figcaption>Figure: Fox Loss Curve</figcaption>
    </figure>

    <h3>Predicted Images Across Iterations (Fox)</h3>
    <p>These images show the reconstruction of the fox image at various training epochs, demonstrating the network’s progress.</p>
    <div class="image-container">
        <figure>
            <img src="media/part_1/fox_pic_1.png" alt="Fox at early epoch" />
            <figcaption>Fox Reconstruction: Early Epoch</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/fox_pic_2.png" alt="Fox intermediate epoch" />
            <figcaption>Fox Reconstruction: Intermediate Epoch</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/fox_pic_4.png" alt="Fox later epoch" />
            <figcaption>Fox Reconstruction: Later Epoch</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/final_fox_pic_reconstructed.png" alt="Fox final reconstructed" />
            <figcaption>Fox Reconstruction: Final</figcaption>
        </figure>
    </div>

    <h3>Second Example: Bear Image</h3>
    <p>
        I then repeated the process on another image (the bear) with a chosen set of hyperparameters. 
        Below is the PSNR curve and a selection of images during training.
    </p>
    <p>PSNR curve for the bear image with a specific learning rate (1e-4):</p>
    <figure>
        <img src="media/part_1/bear_pnsr_learning_rate_1e-4.png" alt="Bear PSNR with LR=1e-4" />
        <figcaption>Bear PSNR Curve with LR=1e-4</figcaption>
    </figure>

    <h3>Bear Reconstruction Across Iterations</h3>
    <div class="image-container">
        <figure>
            <img src="media/part_1/bear_epoch_1.png" alt="Bear epoch 1" />
            <figcaption>Bear Reconstruction: Epoch 1</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/bear_epoch_200.png" alt="Bear epoch 200" />
            <figcaption>Bear Reconstruction: Epoch 200</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/bear_epoch_400.png" alt="Bear epoch 400" />
            <figcaption>Bear Reconstruction: Epoch 400</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/bear_epoch_600.png" alt="Bear epoch 600" />
            <figcaption>Bear Reconstruction: Epoch 600</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/bear_epoch_800.png" alt="Bear epoch 800" />
            <figcaption>Bear Reconstruction: Epoch 800</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/final_bear.png" alt="Bear final reconstructed" />
            <figcaption>Bear Reconstruction: Final</figcaption>
        </figure>
    </div>

    <h3>Hyperparameter Tuning</h3>
    <p>
        I also ran hyperparameter tuning on the bear image. The following plots show loss curves and final results with different learning rates or network settings.
    </p>
    <div class="image-container">
        <figure>
            <img src="media/part_1/bear_learning_rate_1e4_loss_curve.png" alt="Bear LR=1e4 Loss Curve" />
            <figcaption>Bear Loss Curve with LR=1e-4</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/bear_loss.png" alt="Bear Loss with tuned parameters" />
            <figcaption>Bear Loss Curve with Tuned Hyperparameters</figcaption>
        </figure>
    </div>
</section>
<section id="part-2">
    <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
    <p>
        In this part, I used a neural radiance field (NeRF) to represent a 3D scene. Starting start from multi-view calibrated images of a Lego object. 
        I have camera intrinsic and extrinsic parameters that allow us to cast rays into the scene. I then sample points along these rays 
        and feed them into a Neural Radiance Field MLP to predict density and color. Finally, I then volume render the scene 
        to compare against ground truth images and optimize the NeRF parameters.
    </p>

    <h3>Implementation Details</h3>
    <ul>
        <li><strong>Ray and Sample Visualization:</strong> I implemented functions to transform coordinates between camera and world space, 
            and generate rays for each pixel. I also discretize each ray into 3D sample points. The figures below show a visualization of the 
            camera frustums, sampled rays, and sample points.</li>
        <li><strong>MLP Architecture:</strong> The MLP takes as input 3D points (with positional encoding) and view directions (also encoded), 
            and outputs density (via ReLU) and color (via Sigmoid). This deeper network includes skip connections to retain input signal 
            and ensure stable training.</li>
        <li><strong>Volume Rendering:</strong> I implemented the volume rendering equation such that. For each ray, we accumulate colors 
            and densities from sampled points to produce a final pixel color. I also ensure the code passes the provided volume rendering assert test.</li>
        <li><strong>Training Setup:</strong> I as advised by staff's hint use an Adam optimizer with a learning rate of 5e-4 and a batch size of 10K rays per iteration. 
            The staff solution achieves ~23 PSNR after 1000 iterations with these settings.</li>
    </ul>

    <h3>Ray, Camera, and Sample Visualization</h3>
    <p>
        Below is an example visualization of rays and samples I also draw at a single training step, along with the camera poses. 
        I also plot up to 100 rays to keep the visualization less crowded as advised by the deliverables.
    </p>
    <div class="image-container">
        <figure>
            <img src="media/part_2/sampled_rays_1.png" alt="Sampled Rays Visualization 1" />
            <figcaption>Sampled Rays Visualization 1</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/sampled_rays_2.png" alt="Sampled Rays Visualization 2" />
            <figcaption>Sampled Rays Visualization 2</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/sampled_rays_3.png" alt="Sampled Rays Visualization 3" />
            <figcaption>Sampled Rays Visualization 3</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/sampled_rays_4.png" alt="Sampled Rays Visualization 4" />
            <figcaption>Sampled Rays Visualization 4</figcaption>
        </figure>
    </div>

    <h3>Partial Training Iterations Preview</h3>
    <p>
        While I am are still working on the full visualization of the training process (predicted images across more iterations and PSNR curves), 
        here is a quick preview of the model’s output at a few selected iterations:
    </p>
    <div class="image-container">
        <figure>
            <img src="media/part_2/legos_iteration_1.png" alt="Legos Iteration 1" />
            <figcaption>Legos Iteration 1</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/legos_iteration_2.png" alt="Legos Iteration 2" />
            <figcaption>Legos Iteration 2</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/Legos_iteration_3.png" alt="Legos Iteration 3" />
            <figcaption>Legos Iteration 3</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/legos_iteration_4.png" alt="Legos Iteration 4" />
            <figcaption>Legos Iteration 4</figcaption>
        </figure>
    </div>
    <h3>Additional Intermediate Results</h3>
    <p>
        Below are some additional iterations showing the model's progress at different completion percentages. As training goes on, 
        the reconstruction quality improves, and more details become visible:
    </p>
    <div class="image-container">
        <figure>
            <img src="media/part_2/iteration_77%_lego.png" alt="Lego iteration ~77%" />
            <figcaption>Lego Reconstruction around 77% through training</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/lego_88%_iteration.png" alt="Lego iteration ~88%" />
            <figcaption>Lego Reconstruction around 88% through training</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/92%_iteration_lego.png" alt="Lego iteration ~92%" />
            <figcaption>Lego Reconstruction around 92% through training</figcaption>
        </figure>
    </div>
    

    <h3>Novel View Rendering</h3>
    <p>
        After training the network, I used it to render novel views of the Lego scene from arbitrary camera extrinsics. Below are examples 
        of spherical rendering videos showing the Lego from multiple angles. The left video shows the result after 10 minutes of training and 
        the right one after 2.5 minutes of training, demonstrating the improvement in rendering quality over time.
    </p>
    <div class="image-container">
        <figure>
            <video controls width="320">
                <source src="media/part_2/10_minute_final_spherical_lego_renderings.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Spherical Rendering after 10 minutes of training</figcaption>
        </figure>
        <figure>
            <video controls width="320">
                <source src="media/part_2/2.5_minute_nerf_training.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Spherical Rendering after 2.5 hours of training</figcaption>
        </figure>
    </div>

    <h3>Novel View Rendering with Background Bells and Whistles</h3>
    <div class="image-container">
        <figure>
            <video controls width="320">
                <source src="media/part_2/bells_and_whistles_10_minute_final_spherical_rendering_legos.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Spherical Rendering after 10 minutes of training</figcaption>
        </figure>
        <figure>
            <video controls width="320">
                <source src="media/part_2/bells_and_whistles_background_legos_render_2.5_minutes.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Spherical Rendering after 2.5 hours of training</figcaption>
        </figure>
    </div>
</section>
