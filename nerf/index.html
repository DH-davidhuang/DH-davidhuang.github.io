<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 5: Diffusion</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            background-color: #f9f9f9;
        }
        header {
            background-color: #0078D7;
            color: white;
            padding: 20px;
            text-align: center;
        }
        section {
            margin: 20px;
            padding: 20px;
            background-color: white;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        h1, h2, h3 {
            color: #333;
        }
        .image-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 20px;
            margin-top: 20px;
        }
        .image-container img {
            max-width: 45%;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }
        .image-container figure {
            margin: 0;
            text-align: center;
        }
        .image-container figcaption {
            margin-top: 10px;
            font-size: 0.9em;
            color: #555;
        }
    </style>
<section>
    <h2>Part 1 and 2: NERF!</h2>
    <p>
        As a reference, the images below show the process of optimizing the network to fit on the given images. 
        The following results are for a neural field trained on a single input image (the fox), as well as a second example (the bear).
    </p>
    <h3>Model Architecture and Hyperparameters</h3>
    <ul>
        <li>Positional Encoding: L = 10 (total input dimension after encoding: 42)</li>
        <li>MLP Architecture: MLPNeuralField with hidden_dim=256, output_dim=3, num_layers=3</li>
        <li>Loss Function: MSELoss</li>
        <li>Optimizer: Adam with LR=0.001</li>
        <li>Number of Epochs: 1000</li>
    </ul>

    <h3>PSNR Over Training (Fox)</h3>
    <p>Below is the PSNR curve over epochs for the "fox" image training run:</p>
    <figure>
        <img src="media/part_1/fox_pnsr_pic.png" alt="Fox PSNR over epochs" />
        <figcaption>Figure: Fox PSNR Curve</figcaption>
    </figure>
    
    <p>Below is the training loss over epochs for the "fox" image:</p>
    <figure>
        <img src="media/part_1/fox_loss_over_epochs.png" alt="Fox Loss over epochs" />
        <figcaption>Figure: Fox Loss Curve</figcaption>
    </figure>

    <h3>Predicted Images Across Iterations (Fox)</h3>
    <p>These images show the reconstruction of the fox image at various training epochs, demonstrating the network’s progress.</p>
    <div class="image-container">
        <figure>
            <img src="media/part_1/fox_pic_1.png" alt="Fox at early epoch" />
            <figcaption>Fox Reconstruction: Early Epoch</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/fox_pic_2.png" alt="Fox intermediate epoch" />
            <figcaption>Fox Reconstruction: Intermediate Epoch</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/fox_pic_4.png" alt="Fox later epoch" />
            <figcaption>Fox Reconstruction: Later Epoch</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/final_fox_pic_reconstructed.png" alt="Fox final reconstructed" />
            <figcaption>Fox Reconstruction: Final</figcaption>
        </figure>
    </div>

    <h3>Second Example: Bear Image</h3>
    <p>
        I then repeated the process on another image (the bear) with a chosen set of hyperparameters. 
        Below is the PSNR curve and a selection of images during training.
    </p>
    <p>PSNR curve for the bear image with a specific learning rate (1e-4):</p>
    <figure>
        <img src="media/part_1/bear_pnsr_learning_rate_1e-4.png" alt="Bear PSNR with LR=1e-4" />
        <figcaption>Bear PSNR Curve with LR=1e-4</figcaption>
    </figure>

    <h3>Bear Reconstruction Across Iterations</h3>
    <div class="image-container">
        <figure>
            <img src="media/part_1/bear_epoch_1.png" alt="Bear epoch 1" />
            <figcaption>Bear Reconstruction: Epoch 1</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/bear_epoch_200.png" alt="Bear epoch 200" />
            <figcaption>Bear Reconstruction: Epoch 200</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/bear_epoch_400.png" alt="Bear epoch 400" />
            <figcaption>Bear Reconstruction: Epoch 400</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/bear_epoch_600.png" alt="Bear epoch 600" />
            <figcaption>Bear Reconstruction: Epoch 600</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/bear_epoch_800.png" alt="Bear epoch 800" />
            <figcaption>Bear Reconstruction: Epoch 800</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/final_bear.png" alt="Bear final reconstructed" />
            <figcaption>Bear Reconstruction: Final</figcaption>
        </figure>
    </div>

    <h3>Hyperparameter Tuning</h3>
    <p>
        I also ran hyperparameter tuning on the bear image. The following plots show loss curves and final results with different learning rates or network settings.
    </p>
    <div class="image-container">
        <figure>
            <img src="media/part_1/bear_learning_rate_1e4_loss_curve.png" alt="Bear LR=1e4 Loss Curve" />
            <figcaption>Bear Loss Curve with LR=1e-4</figcaption>
        </figure>
        <figure>
            <img src="media/part_1/bear_loss.png" alt="Bear Loss with tuned parameters" />
            <figcaption>Bear Loss Curve with Tuned Hyperparameters</figcaption>
        </figure>
    </div>
</section>
<section id="part-2">
    <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
    <p>
        In this part, I used a neural radiance field (NeRF) to represent a 3D scene. Starting start from multi-view calibrated images of a Lego object. 
        I have camera intrinsic and extrinsic parameters that allow us to cast rays into the scene. I then sample points along these rays 
        and feed them into a Neural Radiance Field MLP to predict density and color. Finally, I then volume render the scene 
        to compare against ground truth images and optimize the NeRF parameters.
    </p>
    <h3>Implementation Details</h3>
    <ul>
        <li><strong>Ray and Sample Visualization:</strong>  
            To build the foundation, I first implemented functions that accurately convert coordinates between camera and world reference frames. 
            Using the camera extrinsics (c2w matrices) and intrinsics (K), I can determine the position and orientation of the camera in the scene. 
            For each pixel in the image, I generate a ray that originates at the camera center and passes through that pixel in 3D space.  
            Once I have these rays, I discretize them into a set of sample points along their path. This involves uniform sampling between a "near" and "far" bound, 
            as well as optional random perturbations to spread out the sampling distribution. The visualizations below depict camera frustums (showing each camera’s field of view), 
            rays extending from these cameras, and the sampled points along those rays. This ensures a comprehensive, spatially distributed set of 3D locations to query from the NeRF model.
        </li>
        <li><strong>MLP Architecture:</strong>  
            The core of the NeRF is a Multi-Layer Perceptron (MLP) that takes in 3D world coordinates along with view directions and predicts both density (σ) and color (RGB). 
            To enable the MLP to represent high-frequency details, I apply positional encoding to both positions and directions, expanding them into a higher-dimensional 
            feature space using sinusoids at various frequencies. The network architecture includes skip connections that re-inject the original encoded input at deeper layers, 
            helping the model retain and reconstruct finer details. The density is predicted via a ReLU layer to ensure non-negativity, and color predictions are passed through 
            a Sigmoid layer to constrain output values between 0 and 1. These design choices follow the original NeRF paper and help in learning a stable, detailed 3D representation.
        </li>
        <li><strong>Volume Rendering:</strong>  
            With density and color predictions for each sampled point, I integrate along the ray to produce a final pixel value. This step implements the volume rendering equation. 
            By accumulating transmittance and absorption through the sampled points, I compute the expected color for each ray. To verify correctness, I test my volume rendering 
            routine against a provided assert statement that checks known outputs for a given set of inputs. Passing this test ensures that the volume rendering logic is implemented correctly.
        </li>
        <li><strong>Training Setup:</strong>  
            For optimization, I follow the staff’s suggested hyperparameters: an Adam optimizer with a learning rate of 5e-4, and a batch size of 10K rays sampled per iteration. 
            Training the NeRF involves iteratively refining the network’s weights so that rendered images match the ground-truth training images from multiple viewpoints. 
            According to the staff solution, these settings can achieve about 23 PSNR after around 1000 iterations. This combination of learning rate, batch size, and iteration count 
            helps ensure a balance between stability, convergence speed, and final image quality.
        </li>
    </ul>
    
    <h3>Ray, Camera, and Sample Visualization</h3>
    <p>
        Below is an example visualization of rays and samples I also draw at a single training step, along with the camera poses. 
        I also plot up to 100 rays to keep the visualization less crowded as advised by the deliverables.
    </p>
    <div class="image-container">
        <figure>
            <img src="media/part_2/sampled_rays_1.png" alt="Sampled Rays Visualization 1" />
            <figcaption>Sampled Rays Visualization 1</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/sampled_rays_2.png" alt="Sampled Rays Visualization 2" />
            <figcaption>Sampled Rays Visualization 2</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/sampled_rays_3.png" alt="Sampled Rays Visualization 3" />
            <figcaption>Sampled Rays Visualization 3</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/sampled_rays_4.png" alt="Sampled Rays Visualization 4" />
            <figcaption>Sampled Rays Visualization 4</figcaption>
        </figure>
    </div>

    <h3>Partial Training Iterations Preview</h3>
    <p>
        While I am are still working on the full visualization of the training process (predicted images across more iterations and PSNR curves), 
        here is a quick preview of the model’s output at a few selected iterations:
    </p>
    <div class="image-container">
        <figure>
            <img src="media/part_2/legos_iteration_1.png" alt="Legos Iteration 1" />
            <figcaption>Legos Iteration 1</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/legos_iteration_2.png" alt="Legos Iteration 2" />
            <figcaption>Legos Iteration 2</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/Legos_iteration_3.png" alt="Legos Iteration 3" />
            <figcaption>Legos Iteration 3</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/legos_iteration_4.png" alt="Legos Iteration 4" />
            <figcaption>Legos Iteration 4</figcaption>
        </figure>
    </div>
    <h3>Additional Intermediate Results</h3>
    <p>
        Below are some additional iterations showing the model's progress at different completion percentages. As training goes on, 
        the reconstruction quality improves, and more details become visible:
    </p>
    <div class="image-container">
        <figure>
            <img src="media/part_2/iteration_77%_lego.png" alt="Lego iteration ~77%" />
            <figcaption>Lego Reconstruction around 77% through training</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/lego_88%_iteration.png" alt="Lego iteration ~88%" />
            <figcaption>Lego Reconstruction around 88% through training</figcaption>
        </figure>
        <figure>
            <img src="media/part_2/92%_iteration_lego.png" alt="Lego iteration ~92%" />
            <figcaption>Lego Reconstruction around 92% through training</figcaption>
        </figure>
    </div>
    

    <h3>Novel View Rendering</h3>
    <p>
        After training the network, I used it to render novel views of the Lego scene from arbitrary camera extrinsics. Below are examples 
        of spherical rendering videos showing the Lego from multiple angles. The left video shows the result after 10 minutes of training and 
        the right one after 2.5 minutes of training, demonstrating the improvement in rendering quality over time.
    </p>
    <div class="image-container">
        <figure>
            <video controls width="320">
                <source src="media/part_2/10_minute_final_spherical_lego_renderings.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Spherical Rendering after 10 minutes of training</figcaption>
        </figure>
        <figure>
            <video controls width="320">
                <source src="media/part_2/2.5_minute_nerf_training.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Spherical Rendering after 2.5 hours of training</figcaption>
        </figure>
    </div>

    <h3>Novel View Rendering with Background Bells and Whistles</h3>
    <div class="image-container">
        <figure>
            <video controls width="320">
                <source src="media/part_2/bells_and_whistles_10_minute_final_spherical_rendering_legos.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Spherical Rendering after 10 minutes of training</figcaption>
        </figure>
        <figure>
            <video controls width="320">
                <source src="media/part_2/bells_and_whistles_background_legos_render_2.5_minutes.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Spherical Rendering after 2.5 hours of training</figcaption>
        </figure>
    </div>
</section>
